import os
import json
import time
from typing import TypedDict
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END

llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="EMPTY",
    model="/models/gpt-oss-120b",
    temperature=0.7
)

# 2. ã€æ–°å¢ã€‘å¿«é€Ÿé€šé“å°ˆç”¨çš„æ¨¡å‹å¯¦ä¾‹
# æŒ‡å‘ä½ æŒ‡å®šçš„ ws-05 URL
fast_llm = ChatOpenAI(
    model="Qwen3-VL-8B-Instruct-BF16.gguf",
    api_key="EMPTY", # å‡è¨­ Key é€šç”¨
    base_url="https://ws-05.huannago.com/v1",
    temperature=0
)

# è¨­å®šå¿«å–æª”æ¡ˆåç¨±
CACHE_FILE = "qa_cache.json"

# ================= å·¥å…·å‡½å¼ (ç¶­æŒåŸæ¨£) =================

def get_clean_key(text: str) -> str:
    """çµ±ä¸€å°‡å•é¡Œæ¨™æº–åŒ–"""
    return text.replace(" ", "").replace("?", "")

def load_cache():
    """å¾ JSON è®€å–å¿«å–è³‡æ–™"""
    if not os.path.exists(CACHE_FILE):
        default_data = {
            get_clean_key("LangGraphæ˜¯ä»€éº¼"): "LangGraph æ˜¯ä¸€å€‹ç”¨æ–¼æ§‹å»ºæœ‰ç‹€æ…‹ã€å¤šåƒèˆ‡è€…æ‡‰ç”¨ç¨‹å¼...",
            get_clean_key("ä½ çš„åå­—"): "æˆ‘æ˜¯é€™å€‹èª²ç¨‹çš„ AI åŠ©æ•™ã€‚"
        }
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(default_data, f, ensure_ascii=False, indent=4)
        return default_data
    
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        return {}

def save_cache(new_data: dict):
    """å°‡è³‡æ–™å¯«å…¥ JSON"""
    current_data = {}
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                current_data = json.load(f)
        except:
            pass
    
    current_data.update(new_data)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(current_data, f, ensure_ascii=False, indent=4)

# ================= LangGraph å®šç¾©å€ =================

# 1. å®šç¾©ç‹€æ…‹
class State(TypedDict):
    question: str
    answer: str
    source: str # CACHE / FAST / LLM

def check_cache_node(state: State):
    """æª¢æŸ¥å¿«å–"""
    print(f"\n[ç³»çµ±] æ”¶åˆ°å•é¡Œ: {state['question']}")
    cache_data = load_cache()
    clean_query = get_clean_key(state['question'])

    if clean_query in cache_data:
        print("--- å‘½ä¸­å¿«å– (Cache Hit) ---")
        return {
            "answer": cache_data[clean_query],
            "source": "CACHE"
        }
    else:
        print("--- å¿«å–æœªå‘½ä¸­ (Cache Miss) ---")
        return {"source": "MISS"}

def fast_reply_node(state: State):
    print("--- é€²å…¥å¿«é€Ÿé€šé“ (Fast Track API) ---")

    response = fast_llm.invoke([HumanMessage(content=state['question'])])

    return {
        "answer": response.content,
        "source": "FAST_TRACK_API"
    }

def expert_node(state: State):
    """
    æ…¢é€Ÿé€šé“ï¼šå‘¼å« LLM ä¸¦ä½¿ç”¨ã€Œæµå¼å‚³è¼¸ã€
    """
    print("--- é€²å…¥å°ˆå®¶æ¨¡å¼ (LLM Expert) ---")

    prompt = f"è«‹ä»¥å°ˆæ¥­çš„è§’åº¦å›ç­”ä»¥ä¸‹å•é¡Œï¼š{state['question']}"

    chunks = llm.stream([HumanMessage(content=prompt)])

    full_answer = ""
    print("ğŸ¤– AI æ­£åœ¨æ€è€ƒä¸¦æ‰“å­—ï¼š", end="", flush=True)

    for chunk in chunks:
        content = chunk.content
        if content:
            print(content, end="", flush=True)
            full_answer += content
    print("\n")

    clean_key = get_clean_key(state['question'])
    save_cache({clean_key: full_answer})
    print(f"--- [ç³»çµ±] å·²å°‡å®Œæ•´å›ç­”å¯«å…¥ {CACHE_FILE} ---")

    return {
        "answer": full_answer,
        "source": "LLM_EXPERT"
    }

def master_router(state: State):
    """ä¸»è·¯ç”±æ§åˆ¶å™¨"""
    if state.get("answer"):
        return "end"

    question = state['question']
    # åªè¦æœ‰é€™äº›æ‰“æ‹›å‘¼çš„è©ï¼Œå°±èµ° Fast Track (ws-05)
    if any(word in question for word in ["ä½ å¥½", "å—¨", "æ—©å®‰", "å“ˆå›‰"]):
        return "fast"
    else:
        return "expert"

workflow = StateGraph(State)

workflow.add_node("check_cache", check_cache_node)
workflow.add_node("fast_bot", fast_reply_node)
workflow.add_node("expert_bot", expert_node)

workflow.set_entry_point("check_cache")

workflow.add_conditional_edges(
    "check_cache",
    master_router,
    {
        "end": END,
        "fast": "fast_bot",
        "expert": "expert_bot"
    }
)

workflow.add_edge("fast_bot", END)
workflow.add_edge("expert_bot", END)

app = workflow.compile()
print(app.get_graph().draw_ascii())

if __name__ == "__main__":
    print(f"å¿«å–æª”æ¡ˆå°‡å„²å­˜æ–¼: {os.path.abspath(CACHE_FILE)}")
    print("æç¤ºï¼šè©¦è‘—è¼¸å…¥ 'ä½ å¥½' æ¸¬è©¦ Fast APIï¼Œè¼¸å…¥å°ˆæ¥­å•é¡Œæ¸¬è©¦ Expert APIã€‚")

    while True:
        user_input = input("\nè«‹è¼¸å…¥å•é¡Œ (è¼¸å…¥ q é›¢é–‹): ")
        if user_input.lower() == 'q':
            break

        inputs = {"question": user_input}

        start_time = time.time()
        try:
            result = app.invoke(inputs)
            end_time = time.time()

            print("-" * 30)
            print(f"ä¾†æº: [{result['source']}]")
            print(f"è€—æ™‚: {end_time - start_time:.4f} ç§’")

            # åªæœ‰é Expert (Cache æˆ– Fast API) éœ€è¦åœ¨é€™è£¡å°å‡ºçµæœ
            if result['source'] != "LLM_EXPERT":
                print(f"å›ç­”: {result['answer']}")
            else:
                print("(å›ç­”å·²æ–¼ä¸Šæ–¹æµå¼è¼¸å‡ºå®Œç•¢)")

        except Exception as e:
            print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
